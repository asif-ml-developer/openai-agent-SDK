{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZOTJuwJcNuhL"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "!pip install -Uq openai-agents\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "gemini_api_key = userdata.get(\"GEMINI_API_KEY\")"
      ],
      "metadata": {
        "id": "I3Wv5DjkplWK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "M1a3NkvLzGZR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# agent_level\n",
        "\n",
        "from agents import Agent, OpenAIChatCompletionsModel, Runner, **set_tracing_disabled**\n",
        "\n",
        "*   provider\n",
        "\n",
        "**set_tracing_disabled(disabled=True)**\n",
        "\n",
        "agent = Agent(\n",
        "        name=\"Assistant\",\n",
        "        instructions=\"You only respond in haikus.\",\n",
        "        **model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=provider),**\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "Vp59XBX9yPfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from openai import AsyncOpenAI\n",
        "from agents import Agent, OpenAIChatCompletionsModel, Runner, set_tracing_disabled\n",
        "\n",
        "# gemini_api_key = \"\"\n",
        "\n",
        "#Reference: https://ai.google.dev/gemini-api/docs/openai\n",
        "provider = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ") # only provider in this\n",
        "\n",
        "set_tracing_disabled(disabled=True)\n",
        "\n",
        "async def main():\n",
        "    # This agent will use the custom LLM provider\n",
        "    agent = Agent(\n",
        "        name=\"Assistant\",\n",
        "        instructions=\"You only respond in haikus.\",\n",
        "        model=OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\", openai_client=provider), # model describe in agent\n",
        "    )\n",
        "\n",
        "    result = await Runner.run(\n",
        "        agent,\n",
        "        \"Tell me about recursion in programming.\",\n",
        "    )\n",
        "    print(result.final_output)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT3iJh_gyGdm",
        "outputId": "8be7a8a9-c641-46e9-f440-75950b0ca359"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A function calls self,\n",
            "Solving smaller problems,\n",
            "Base case stops the call.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run_Level\n",
        "\n",
        "\n",
        "*   provider\n",
        "*   model\n",
        "*   config(**tracing_disabled=True**)\n",
        "\n",
        "result = Runner.run_sync(agent, \"Hello, how are you.\", **run_config=config**)\n"
      ],
      "metadata": {
        "id": "PvLy3yenyUUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\n",
        "from agents.run import RunConfig\n",
        "\n",
        "# gemini_api_key = \"\"\n",
        "\n",
        "#Reference: https://ai.google.dev/gemini-api/docs/openai\n",
        "provider = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=provider\n",
        ")\n",
        "\n",
        "config = RunConfig(\n",
        "    model=model,\n",
        "    model_provider=provider,\n",
        "    tracing_disabled=True # tracing_dsiabled is equal to True\n",
        ")\n",
        "\n",
        "agent: Agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\")\n",
        "\n",
        "result = Runner.run_sync(agent, \"Hello, how are you.\", run_config=config) # run_config is used at run time\n",
        "\n",
        "print(result.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SneBati-yX4w",
        "outputId": "60928b57-4323-4da5-d110-c42e76e6b743"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! As a large language model, I don't experience feelings like humans do, so I don't \"feel\" in the same way. However, I am functioning properly and ready to assist you. How can I help you today?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global_Level\n",
        "\n",
        "from agents import **set_default_openai_client**\n",
        "**set_default_openai_client(external_client)**\n",
        "\n",
        "*   provider\n",
        "*   model\n",
        "\n",
        "agent: Agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\", **model=model**)"
      ],
      "metadata": {
        "id": "IDIwyWgXyZ9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\n",
        "from agents import set_default_openai_client # import set default for all project\n",
        "\n",
        "# gemini_api_key = \"\"\n",
        "\n",
        "provider = AsyncOpenAI(\n",
        "    api_key=gemini_api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        ")\n",
        "set_default_openai_client(provider)  # set default for all project\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    openai_client=provider\n",
        ")\n",
        "\n",
        "agent: Agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\", model=model) #  call model here\n",
        "\n",
        "result = Runner.run_sync(agent, \"Hello, how are you.\")# no config file\n",
        "\n",
        "print(result.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTzdtQWLyfDq",
        "outputId": "a121ebfb-25a8-4783-d678-5c4b92d82d5b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! As a large language model, I don't experience feelings in the same way humans do. But I'm functioning well and ready to assist you. How can I help you today?\n",
            "\n"
          ]
        }
      ]
    }
  ]
}